{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS =  Overfitting occurs when a model learns to capture noise or random fluctuations in the training data, resulting in poor generalization to unseen data. Underfitting, on the other hand, happens when a model is too simple to capture the underlying patterns in the data, leading to poor performance on both training and test data. Overfitting can be mitigated by using techniques like cross-validation, regularization, and increasing training data. Underfitting can be addressed by using more complex models, adding more features, or reducing regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  =  To reduce overfitting, one can use techniques such as cross-validation, regularization (e.g., L1, L2 regularization), early stopping, pruning (for decision trees), and ensemble methods like bagging and boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS = Underfitting occurs when a model is too simple to capture the underlying patterns in the data. It can happen when the model complexity is low, the training data is insufficient, or the features are not informative enough. Scenarios where underfitting can occur include using a linear model for highly non-linear data, using a low-degree polynomial for polynomial regression, or using a shallow neural network for complex tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS = The bias-variance tradeoff refers to the balance between bias (error due to the model's assumptions being too simplistic) and variance (error due to the model's sensitivity to fluctuations in the training data). Models with high bias tend to underfit the data, while models with high variance tend to overfit. The goal is to find the right balance between bias and variance to achieve optimal model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  =   Common methods for detecting overfitting and underfitting include analyzing learning curves, examining performance metrics on training and test data, using techniques like cross-validation, and visualizing decision boundaries or regression curves. Overfitting may be indicated by a large gap between training and test error, while underfitting may be indicated by high error on both training and test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias refers to the error introduced by approximating a real-world problem with a simplified model. High bias models tend to underfit the data. Variance refers to the model's sensitivity to fluctuations in the training data. High variance models tend to overfit the data. Examples of high bias models include linear regression with few features, while examples of high variance models include deep neural networks with many layers trained on limited data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization in machine learning is a technique used to prevent overfitting by adding a penalty term to the loss function, discouraging overly complex models. Common regularization techniques include L1 regularization (Lasso), L2 regularization (Ridge), dropout regularization (for neural networks), and elastic net regularization (a combination of L1 and L2 regularization). Regularization helps in controlling model complexity and improving generalization to unseen data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
